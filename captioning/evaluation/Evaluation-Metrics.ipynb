{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sequence_candidate import SequenceCandidate\n",
    "from beam_predict import generate_predictions_beam\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/features/valid_features.pkl\", \"rb\") as handle:\n",
    "    valid_features = pickle.load(handle)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features[\"_ExrVJTjGcChfzLH51etAw\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_captions = pd.read_csv(\"../data/split_lists/valid_ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tokenizer/tokenizer.pkl\",\"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_tokenizer = {index: word for word,index in tokenizer.word_index.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_concat = load_model(\"models/model_merge-date_5-18-14-40-ep016-loss4.704_lr-0.010000_patience-3.h5\")\n",
    "merge_add = load_model(\"models/model_merge_add-date_6-4-14-11-ep014-loss4.864_lr-0.010000_patience-3.h5\")\n",
    "inject = load_model(\"models/model_inject-date_5-16-15-45-ep030-loss5.009_lr-0.010000_patience-3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_concat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_add.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inject.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = valid_captions.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(int(.9*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions(df):\n",
    "    models = [merge_concat, merge_add, inject]\n",
    "    model_names = [\"merge_concat\", \"merge_add\", \"inject\"]\n",
    "    alpha_range = [.6, .7, .8]\n",
    "    for i in range(len(models)):\n",
    "        for alpha in alpha_range:\n",
    "            colname = \"_\".join([model_names[i], str(int(alpha*10)), 'pred'])\n",
    "            df[colname] = df.photo_id.apply(lambda x:\n",
    "                            generate_predictions_beam(img_id = x, features= valid_features, \n",
    "                                caption_model=models[i], \n",
    "                                  reverse_tokenizer=reverse_tokenizer,\n",
    "                                  width = 3, num_neighbors = 5, top_n = 1,\n",
    "                                  alpha = alpha)[0][0])\n",
    "            print(\"done: alpha = %f, model = %s, time: %s\" %(alpha, model_names[i], str(datetime.datetime.now())))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_full = add_predictions(valid_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_csv(\"df_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu1(ref, cand):\n",
    "    return sentence_bleu([ref.split()], cand.split(), weights=(1.0, 0, 0, 0))\n",
    "\n",
    "def bleu2(ref, cand):\n",
    "    return sentence_bleu([ref.split()], cand.split(), weights=(.5, .5, 0, 0))\n",
    "\n",
    "def bleu3(ref, cand):\n",
    "    return sentence_bleu([ref.split()], cand.split(), weights=(.33, .33, .33, 0))\n",
    "\n",
    "def bleu4(ref, cand):\n",
    "    return sentence_bleu([ref.split()], cand.split(), weights=(.25, .25, .25, .25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cols = df_full.columns[2:]\n",
    "print(pred_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full.apply(lambda row: bleu1(str(row[\"caption\"]), str(row[\"merge_concat_6_pred\"])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bleu(df):\n",
    "    bleu_funcs = [bleu1, bleu2, bleu3, bleu4]\n",
    "    for col in pred_cols:\n",
    "        for ngram in range(1,5):\n",
    "            colname = \"%s_bleu%d\"%(col[:-5], ngram)\n",
    "            bleu_func = bleu_funcs[ngram-1]\n",
    "            df[colname] = df.apply(lambda row: bleu_func(str(row[\"caption\"]), str(row[col])), axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = add_bleu(df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the']\n",
    "sentence_bleu(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bleu([\"grilled chicken salad\".split()] , \"shanghai rainbow chicken\".split(), weights=(1.0, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_full.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.iloc[85].caption"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}